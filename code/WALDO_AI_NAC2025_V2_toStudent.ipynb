{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 **Waldo AI: The Ultimate Hide-and-Seek Showdown**\n",
        "\n",
        "[![MARS AI](https://github.com/kaopanboonyuen/kaopanboonyuen.github.io/raw/main/files/logo/NAC2025_WaldoAI.png)](https://github.com/kaopanboonyuen/where-is-waldo)\n",
        "\n",
        "## By Kao Panboonyuen\n",
        "\n",
        "### This Colab notebook will guide you through:\n",
        "\n",
        "* ✅ Preparing and loading the dataset\n",
        "* ✅ Exploring images and labels\n",
        "* ✅ Performing exploratory data analysis (EDA)\n",
        "* ✅ Training AI (small model) for segmentation\n",
        "* ✅ Evaluating performance with accuracy, confusion matrix, precision, recall, F1-score\n",
        "* ✅ Performing inference and error analysis"
      ],
      "metadata": {
        "id": "uYjUn9MHXc3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 **Step 1: Install & Import Required Libraries**\n",
        "\n",
        "In this step, we'll install the necessary libraries for training YOLOv12. You'll need to install the YOLOv12 package, and PyTorch to enable GPU acceleration."
      ],
      "metadata": {
        "id": "sHWaV3jmXiIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTudJX8OXaTh"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.0.0+cu117 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torchvision"
      ],
      "metadata": {
        "id": "4ecNWk7uZylw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "E_VVz9jmXu9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA (GPU support) is available for PyTorch and print the result (True/False).\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# Print the number of CUDA-compatible devices (GPUs) available on the system.\n",
        "print(torch.cuda.device_count())\n",
        "\n",
        "# Print the name of the first CUDA-compatible device (GPU) available (index 0)."
      ],
      "metadata": {
        "id": "_khyiWBxXupw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "FuCivgmicQQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the PyTorch library, which provides tools for machine learning and deep learning."
      ],
      "metadata": {
        "id": "IDgR9l0nYTMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "E8XtAGGhcSMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the YOLO (You Only Look Once) model from the 'ultralytics' package for object detection tasks."
      ],
      "metadata": {
        "id": "TaKws3R_cNgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "CTyKY2W9cTFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 **Step 2: Download and Prepare the Dataset**\n",
        "\n",
        "Next, we will download the dataset from the link provided and extract it."
      ],
      "metadata": {
        "id": "XbEbBrOiXtIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Optional Dataset (Type: A)\n",
        "\n",
        "# # Define the global variables for the developer name and repository name\n",
        "# DEV_NAME = 'kaopanboonyuen'\n",
        "# REPO_NAME = 'where-is-waldo'\n",
        "\n",
        "# # Use f-string to create the dynamic URL for the dataset\n",
        "# url = f'https://github.com/{DEV_NAME}/{REPO_NAME}/raw/main/dataset/waldo-dataset-a.zip'\n",
        "\n",
        "# # Download the zip file using wget\n",
        "# # !wget https://github.com/{DEV_NAME}/{REPO_NAME}/raw/main/dataset/waldo-dataset-a.zip -O waldo-dataset-a.zip\n",
        "# !wget {url} -O waldo-dataset-a.zip\n",
        "\n",
        "# # Unzip the downloaded file\n",
        "# # !unzip /content/waldo-dataset-a.zip >> logs.log\n",
        "# !unzip /content/waldo-dataset-a.zip >> logs.log"
      ],
      "metadata": {
        "id": "e7DJ87_XXXUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the global variable for the repository name\n",
        "DEV_NAME = # Write your code here\n",
        "REPO_NAME = # Write your code here\n",
        "\n",
        "# Use f-string to create the dynamic URLs\n",
        "url_part1 = f'https://github.com/{DEV_NAME}/{REPO_NAME}/raw/main/dataset/waldo-dataset-b.zip.part1'\n",
        "url_part2 = f'https://github.com/{DEV_NAME}/{REPO_NAME}/raw/main/dataset/waldo-dataset-b.zip.part2'\n",
        "url_part3 = f'https://github.com/{DEV_NAME}/{REPO_NAME}/raw/main/dataset/waldo-dataset-b.zip.part3'\n",
        "url_part4 = f'https://github.com/{DEV_NAME}/{REPO_NAME}/raw/main/dataset/waldo-dataset-b.zip.part4'\n",
        "url_part5 = f'https://github.com/{DEV_NAME}/{REPO_NAME}/raw/main/dataset/waldo-dataset-b.zip.part5'\n",
        "\n",
        "# Use wget with the generated URLs for each part\n",
        "!wget {url_part1} -O waldo-dataset-b.zip.part1\n",
        "!wget {url_part2} -O waldo-dataset-b.zip.part2\n",
        "!wget {url_part3} -O waldo-dataset-b.zip.part3\n",
        "!wget {url_part4} -O waldo-dataset-b.zip.part4\n",
        "!wget {url_part5} -O waldo-dataset-b.zip.part5"
      ],
      "metadata": {
        "id": "XkeQ2IDY8WrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reassemble_zip(parts, output_file):\n",
        "    with open(output_file, 'wb') as output_zip:\n",
        "        for part in parts:\n",
        "            with open(part, 'rb') as part_file:\n",
        "                output_zip.write(part_file.read())\n",
        "    print(f\"Reassembled to {output_file}.\")\n",
        "\n",
        "parts = ['waldo-dataset-b.zip.part1', 'waldo-dataset-b.zip.part2',\n",
        "         'waldo-dataset-b.zip.part3', 'waldo-dataset-b.zip.part4', 'waldo-dataset-b.zip.part5']  # List of split parts\n",
        "output_file = 'waldo-dataset-b.zip'  # Output ZIP file"
      ],
      "metadata": {
        "id": "Ce7GteeC7_KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call Reassemble the parts of a split ZIP file into a single ZIP file.\n",
        "\n",
        "# Write your code here"
      ],
      "metadata": {
        "id": "qpAvGDX2ZReB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('write-your-file-name.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "metadata": {
        "id": "MI-Kfdk6Xsur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 **Step 3: Organize Data in YOLOv12 Format**\n",
        "\n",
        "YOLOv12 requires the data to be organized in a specific format. We'll make sure that the labels and images are correctly set up."
      ],
      "metadata": {
        "id": "uOkY2k58YA0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/dataset', exist_ok=True)\n",
        "os.makedirs('/content/dataset/train', exist_ok=True)\n",
        "os.makedirs('/content/dataset/valid', exist_ok=True)\n",
        "\n",
        "shutil.move('/content/waldo-dataset-b/train/images', '/content/dataset/train/images')\n",
        "shutil.move('/content/waldo-dataset-b/train/labels', '/content/dataset/train/labels')\n",
        "shutil.move('/content/waldo-dataset-b/valid/images', '/content/dataset/valid/images')\n",
        "shutil.move('/content/waldo-dataset-b/valid/labels', '/content/dataset/valid/labels')"
      ],
      "metadata": {
        "id": "vVE9zyVLYBIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 Step 4: Preview Dataset with Random Image Samples"
      ],
      "metadata": {
        "id": "YBODEZ5NYPoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def show_random_label_overlay(train_images_dir, train_labels_dir, num_images=4):\n",
        "    \"\"\"\n",
        "    Function to show random images with their labels overlaid in a grid (1 row, 4 columns).\n",
        "\n",
        "    :param train_images_dir: Path to the directory containing the images\n",
        "    :param train_labels_dir: Path to the directory containing the labels\n",
        "    :param num_images: Number of random images to show in the grid (default is 4)\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1, num_images, figsize=(15, 5))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Randomly select an image and its corresponding label\n",
        "        image_file = random.choice(os.listdir(train_images_dir))\n",
        "        label_file = image_file.replace('.jpg', '.txt')  # Assuming the labels are .txt files with the same name as the image\n",
        "\n",
        "        # Load the image\n",
        "        image = cv2.imread(os.path.join(train_images_dir, image_file))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load the label file\n",
        "        label_path = os.path.join(train_labels_dir, label_file)\n",
        "        with open(label_path, 'r') as file:\n",
        "            labels = file.readlines()\n",
        "\n",
        "        # Loop through each label and draw the bounding box on the image\n",
        "        for label in labels:\n",
        "            parts = label.strip().split()\n",
        "            class_id = int(parts[0])\n",
        "            x_center, y_center, width, height = map(float, parts[1:])\n",
        "\n",
        "            # Convert normalized coordinates to pixel values\n",
        "            img_height, img_width, _ = image.shape\n",
        "            x_center = int(x_center * img_width)\n",
        "            y_center = int(y_center * img_height)\n",
        "            width = int(width * img_width)\n",
        "            height = int(height * img_height)\n",
        "\n",
        "            # Calculate the top-left and bottom-right corners of the bounding box\n",
        "            x1 = x_center - width // 2\n",
        "            y1 = y_center - height // 2\n",
        "            x2 = x_center + width // 2\n",
        "            y2 = y_center + height // 2\n",
        "\n",
        "            # Draw the bounding box on the image\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Red color for bounding box\n",
        "\n",
        "        # Display the image with bounding boxes overlaid\n",
        "        axs[i].imshow(image)\n",
        "        axs[i].axis('off')  # Turn off axis\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8QzbBAK1YP30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to show random images with label overlays"
      ],
      "metadata": {
        "id": "hdX4LIEbQSsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here"
      ],
      "metadata": {
        "id": "WEjXCflUcgwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 **Step 5: Configure the Dataset YAML File**\n",
        "\n",
        "For YOLOv12 to know how to process the dataset, we need to create a `data.yaml` configuration file. This file specifies where the dataset is located and defines the class names."
      ],
      "metadata": {
        "id": "e-3OPFB4ZWPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## FOR WALDO-A DATASET\n",
        "\n",
        "# data_yaml = \"\"\"\n",
        "# train: /content/dataset/train/images\n",
        "# val: /content/dataset/valid/images\n",
        "\n",
        "# nc: 4\n",
        "# names: ['odlaw', 'wally', 'wenda', 'wizard_whitebeard']\n",
        "# \"\"\"\n",
        "# with open(\"/content/dataset/data.yaml\", \"w\") as f:\n",
        "#     f.write(data_yaml)"
      ],
      "metadata": {
        "id": "KivRGnJnkrwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FOR WALDO-B DATASET\n",
        "\n",
        "data_yaml = \"\"\"\n",
        "train: # Write your path here\n",
        "val: # Write your path here\n",
        "\n",
        "nc: # Write your code here\n",
        "names: # Write your code here\n",
        "\"\"\"\n",
        "with open(\"/content/dataset/data.yaml\", \"w\") as f:\n",
        "    f.write(data_yaml)"
      ],
      "metadata": {
        "id": "E6VymmLiZR7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 **Step 6: Load the YOLOv12 Model**\n",
        "\n",
        "Now, load the YOLOv12 small model (YOLOv12n) from the ultralytics package."
      ],
      "metadata": {
        "id": "5V0KMPVxZZHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the pretrained YOLOv12 model (if available)\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "xjlo2a90cqG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained model (adjust the URL or model name if necessary)\n",
        "model = YOLO(\"Write your model here\")  # Replace this with the correct path or model"
      ],
      "metadata": {
        "id": "Id2RrvREZXun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 **Step 7: Train the Model**\n",
        "\n",
        "Now that we have everything ready, we can start training the model. We will fine-tune the pre-trained YOLOv12 small model on the pothole dataset."
      ],
      "metadata": {
        "id": "GSHXK5NCZckH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your trainer code here"
      ],
      "metadata": {
        "id": "KvBFFx9FZar0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 **Step 8: Evaluate the Model**\n",
        "\n",
        "Once training is complete, evaluate the model's performance on the validation dataset."
      ],
      "metadata": {
        "id": "3qswMQFBa2J7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = # Write your code here  # Evaluate on validation set"
      ],
      "metadata": {
        "id": "h1CXUTu0ZeKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 **Step 9: Inference and Error Analysis**\n",
        "\n",
        "After evaluation, it's time to run inference on some images and analyze the errors. Let’s run inference on a few images and visualize the predictions."
      ],
      "metadata": {
        "id": "8__-MHXga5Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform inference on validation images\n",
        "results = # Write your code here"
      ],
      "metadata": {
        "id": "y2L-lehXa39m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = YOLO('# Write your best of AI model path here')  # Replace with the correct path to your trained model\n",
        "\n",
        "# Perform inference on the image\n",
        "img_path = 'Write your inference path here'\n",
        "results = model(img_path)\n",
        "\n",
        "# Access the first result from the list (since results is a list)\n",
        "result = results[0]\n",
        "\n",
        "# Render the results (bounding boxes, labels, and confidence scores)\n",
        "# Write your code here  # This will display the image with bounding boxes and labels overlaid"
      ],
      "metadata": {
        "id": "tV-WHgaKa-Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 **Step 10: Confusion Matrix and Precision-Recall-F1 Analysis**\n",
        "\n",
        "Evaluate the predictions using metrics like Precision, Recall, and F1-Score:"
      ],
      "metadata": {
        "id": "MujbXCm2bPSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Path to the confusion matrix image\n",
        "confusion_matrix_path = 'Write your CF matrix image here'\n",
        "\n",
        "# Display the confusion matrix image\n",
        "display(Image(filename=confusion_matrix_path))"
      ],
      "metadata": {
        "id": "p_gNfvFObPgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📌 **Step 11: Summary of Waldo Faces Detected**\n",
        "\n",
        "🎉 **Yeah, it's the final step!** We've been on an exciting journey, and now it's time to find out who found the most Waldo faces! 🕵️‍♂️"
      ],
      "metadata": {
        "id": "Uw5ArEXCvNuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEV_NAME = # Write your code here\n",
        "REPO_NAME = # Write your code here\n",
        "\n",
        "# Use f-string to create the dynamic URL\n",
        "url = f'https://github.com/{DEV_NAME}/{REPO_NAME}/raw/main/dataset/waldo-dataset-test.zip'\n",
        "\n",
        "# Now use wget with the generated URL\n",
        "!wget {url} -O waldo-dataset-test.zip"
      ],
      "metadata": {
        "id": "H2tsiuRpvsgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('Write your file here', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "metadata": {
        "id": "uYy-KbVrsIhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Load YOLO model with the best weights\n",
        "model = YOLO('Write your best of AI model here')\n",
        "\n",
        "# Path to the test dataset\n",
        "test_images_dir = Path('Write your path here')\n",
        "\n",
        "# Initialize counters\n",
        "total_waldo_faces = 0\n",
        "\n",
        "# Loop through the test images\n",
        "for img_path in test_images_dir.glob('*.jpg'):\n",
        "    print(f\"Processing {img_path.name}... 🕵️‍♂️\")\n",
        "\n",
        "    # Read the image\n",
        "    img = cv2.imread(str(img_path))\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Make predictions with YOLO\n",
        "    results = model(img_rgb)\n",
        "\n",
        "    # Extract bounding boxes and labels\n",
        "    pred = results[0]  # Get the first result from the list\n",
        "    boxes = pred.boxes.xywh  # Box coordinates (x, y, width, height)\n",
        "    names = pred.names  # Class names\n",
        "\n",
        "    # Loop through predictions and filter for 'waldorotation' and 'wendarotation'\n",
        "    waldo_faces = []\n",
        "    for i, box in enumerate(boxes):\n",
        "        label = pred.boxes.cls[i].item()  # Access the class index for this detection\n",
        "        if names[label] in ['waldorotation', 'wendarotation']:  # Check if it's either 'waldorotation' or 'wendarotation'\n",
        "            waldo_faces.append(box)\n",
        "\n",
        "    # Count the number of Waldo faces found\n",
        "    waldo_count = len(waldo_faces)\n",
        "    total_waldo_faces += waldo_count\n",
        "\n",
        "    # Draw the bounding boxes directly on the original image (img_rgb)\n",
        "    for box in waldo_faces:\n",
        "        x, y, w, h = box\n",
        "        x1, y1, x2, y2 = int(x - w / 2), int(y - h / 2), int(x + w / 2), int(y + h / 2)\n",
        "        cv2.rectangle(img_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    # Show the image with bounding boxes overlayed\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(f\"{img_path.name} - Found {waldo_count} Waldo Face{'s' if waldo_count != 1 else ''} 😎\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Summary after processing all images\n",
        "print(f\"\\n🎉🎉🎉 Total Waldo Faces Found: {total_waldo_faces} 😎\")\n",
        "print(\"The winner will be the student who finds the most Waldo faces! 🏆\")"
      ],
      "metadata": {
        "id": "EPDTGlgCqVkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DONE"
      ],
      "metadata": {
        "id": "0wCsbWHrXL2f"
      }
    }
  ]
}